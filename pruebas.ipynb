{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar(texto):\n",
    "  stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "  # Tokenizar la frase\n",
    "  #tokens = word_tokenize(texto)\n",
    "  patron = r'\\w+|[^\\w\\s]'\n",
    "  tokens = regexp_tokenize(texto, patron)\n",
    "  stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "  # Eliminar las palabras vacías\n",
    "  tokens_limpios =  [ stemmer.stem(token.lower()) for token in tokens if token.isalnum() and token not in stop_words ]\n",
    "  \n",
    "  return tokens_limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similitud_coseno(query, document):\n",
    "    query_tokens = procesar(query)\n",
    "    document_tokens = procesar(document)\n",
    "    \n",
    "    # Calcular la frecuencia de las palabras en la consulta y el documento\n",
    "    query_freq = dict(nltk.FreqDist(query_tokens))\n",
    "    document_freq = dict(nltk.FreqDist(document_tokens))\n",
    "        \n",
    "    # Obtener un conjunto único de todas las palabras en la consulta y el documento\n",
    "    all_words = set(query_freq.keys()).union(set(document_freq.keys()))\n",
    "    \n",
    "    # Crear vectores de frecuencia para la consulta y el documento\n",
    "    query_vector = np.array([query_freq.get(word, 0) for word in all_words])\n",
    "    document_vector = np.array([document_freq.get(word, 0) for word in all_words])\n",
    "    \n",
    "    # Calcular la similitud coseno entre los vectores de frecuencia\n",
    "    dot_product = np.dot(query_vector, document_vector)\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    document_norm = np.linalg.norm(document_vector)\n",
    "    similarity = dot_product / (query_norm * document_norm)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def procesar2(texto):\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "    # Tokenizar la frase\n",
    "    tokens = word_tokenize(texto)\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "    # Eliminar las palabras vacías y aplicar stemming\n",
    "    tokens_limpios =  [ stemmer.stem(token.lower()) for token in tokens if token.isalnum() and token not in stop_words ]\n",
    "\n",
    "    texto_procesado = ' '.join(tokens_limpios)\n",
    "    return texto_procesado\n",
    "\n",
    "def similitud_coseno2(query, document):\n",
    "    # Preprocesar los textos\n",
    "    query_procesado = procesar2(query)\n",
    "    document_procesado = procesar2(document)\n",
    "\n",
    "    # Crea el vectorizador de características\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Vectoriza los textos\n",
    "    vector_query = vectorizer.fit_transform([query_procesado])\n",
    "    vector_document = vectorizer.transform([document_procesado])\n",
    "\n",
    "    # Calcula la similitud coseno entre los vectores de frecuencia\n",
    "    similarity = cosine_similarity(vector_query, vector_document)\n",
    "\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6963106238227914, 0.9428090415820636)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"peliculas de accion y aventura\"\n",
    "doc = \"pelicula de accion donde los personajes viven grandes aventuras por todo el mundo como si fuese una pelicula\"\n",
    "\n",
    "sm1 = similitud_coseno(query, doc)\n",
    "sm2 = similitud_coseno2(query, doc)\n",
    "\n",
    "\n",
    "if sm1 == sm2: print(\"Son iguales\")\n",
    "sm1, sm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"f:/UCI/Mis Proyectos/Proyecto_BuscadorInteligente (React y Django-Rest-Framework)/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idfs(vocabulary, doc_features):\n",
    "  # crea un diccionario vacío para almacenar los IDFs\n",
    "  doc_idfs = {}\n",
    "  # itera por cada término del vocabulario\n",
    "  for term in vocabulary:\n",
    "    doc_count = 0\n",
    "    # itera por cada documento en el conjunto de documentos\n",
    "    for doc_id in doc_features.keys():\n",
    "      # obtiene los términos del documento actual\n",
    "      terms = doc_features.get(doc_id)\n",
    "      # si el término actual aparece en el documento actual, aumenta el contador\n",
    "      if term in terms.keys():\n",
    "        doc_count += 1\n",
    "    # calcula el IDF del término actual y lo almacena en el diccionario de IDFs\n",
    "    doc_idfs[term] = math.log(\n",
    "      float(len(doc_features.keys()))/\n",
    "      float(1 + doc_count), 10)\n",
    "  # devuelve el diccionario de IDFs\n",
    "  return doc_idfs\n",
    "\n",
    "def calculate_tf_idf(corpus):\n",
    "  # crea un diccionario para almacenar los términos y su frecuencia en cada documento\n",
    "  doc_features = {}\n",
    "  # crea una lista para almacenar todos los términos únicos en todos los documentos\n",
    "  vocabulary = []\n",
    "  # itera por cada documento en el corpus\n",
    "  for doc_id, doc in corpus.items():\n",
    "    # crea un diccionario vacío para almacenar los términos y su frecuencia en el documento actual\n",
    "    term_freq = {}\n",
    "    # itera por cada palabra en el documento actual\n",
    "    for word in doc.split():\n",
    "      # si la palabra actual ya está en el diccionario de términos y frecuencias, aumenta su frecuencia en 1\n",
    "      if word in term_freq.keys():\n",
    "        term_freq[word] += 1\n",
    "      # si la palabra actual no está en el diccionario de términos y frecuencias, agrega una nueva entrada con una frecuencia de 1\n",
    "      else:\n",
    "        term_freq[word] = 1\n",
    "        # si la palabra actual no está en la lista de vocabulario, agréguela\n",
    "        if word not in vocabulary:\n",
    "          vocabulary.append(word)\n",
    "    # agrega el diccionario de términos y frecuencias del documento actual al diccionario de características de documentos\n",
    "    doc_features[doc_id] = term_freq\n",
    "  \n",
    "  # calcula los valores IDF para cada término en el vocabulario\n",
    "  idfs = calculate_idfs(vocabulary, doc_features)\n",
    "  \n",
    "  # crea un diccionario para almacenar los vectores de características de cada documento\n",
    "  doc_vectors = {}\n",
    "  # itera por cada documento en el corpus\n",
    "  for doc_id, doc in corpus.items():\n",
    "    # crea un vector de características vacío para el documento actual\n",
    "    doc_vector = []\n",
    "    # itera por cada término en el vocabulario\n",
    "    for term in vocabulary:\n",
    "      # si el término actual está presente en el documento actual, calcula su puntaje TF-IDF y lo agrega al vector de características\n",
    "      if term in doc_features[doc_id].keys():\n",
    "        tf_idf = doc_features[doc_id][term] * idfs[term]\n",
    "        doc_vector.append(tf_idf)\n",
    "      # si el término actual no está presente en el documento actual, agrega un 0 al vector de características\n",
    "      else:\n",
    "        doc_vector.append(0)\n",
    "    # agrega el vector de características del documento actual al diccionario de vectores de características\n",
    "    doc_vectors[doc_id] = doc_vector\n",
    "  \n",
    "  # devuelve el diccionario de vectores de características\n",
    "  return doc_vectors\n",
    "\n",
    "corpus = {\n",
    "  'doc1': 'hello world',\n",
    "  'doc2': 'hello python',\n",
    "  'doc3': 'python is awesome',\n",
    "  'doc4': 'world is awesome'\n",
    "}\n",
    "\n",
    "doc_vectors = calculate_tf_idf(corpus)\n",
    "print(doc_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
